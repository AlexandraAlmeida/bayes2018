---
title: "BIDA in Stan: Piecewise constant hazard Cox"
author: "Kazuki Yoshida"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output: html_document
---

```{r, message = FALSE, tidy = FALSE, echo = F}
## knitr configuration: http://yihui.name/knitr/options#chunk_options
library(knitr)
showMessage <- FALSE
showWarning <- TRUE
set_alias(w = "fig.width", h = "fig.height", res = "results")
opts_chunk$set(comment = "##", error= TRUE, warning = showWarning, message = showMessage,
               tidy = FALSE, cache = F, echo = T,
               fig.width = 7, fig.height = 7, dev.args = list(family = "sans"))
## for rgl
## knit_hooks$set(rgl = hook_rgl, webgl = hook_webgl)
## for animation
opts_knit$set(animation.fun = hook_ffmpeg_html)

## R configuration
options(width = 116, scipen = 5)
```

## References
- Online
  - [Stan Leukemia example](https://github.com/stan-dev/example-models/blob/master/bugs_examples/vol1/leuk/leuk.stan)
  - [Stan for survival models](https://discourse.mc-stan.org/t/stan-for-survival-models/4146)
  - [PyMC3 Bayesian Survival Analysis](https://docs.pymc.io/notebooks/survival_analysis.html)
  - [Piece-Wise Exponential Model](http://data.princeton.edu/wws509/notes/c7s4.html)
  - [Stan-dev Prior Choice Recommendations](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations)
- Books
  - [(BIDA) Bayesian Ideas and Data Analysis An Introduction for Scientists and Statisticians](http://blogs.oregonstate.edu/bida/)
  - [(BUGS) The BUGS Book: A Practical Introduction to Bayesian Analysis](https://www.mrc-bsu.cam.ac.uk/software/bugs/the-bugs-project-the-bugs-book/)
  - [(BSA) Bayesian Survival Analysis](https://www.springer.com/us/book/9780387952772)


## Background

The proportional hazards (PH) model (Cox 1972) is a very popular regression method for survival data. The popularity likely originated in not having to specify the (baseline) hazard function. These days, it is popular probably because it is the only survival regression model that many applied researchers are aware of. In Frequentist PH model, parameter estimation is conducted via partial likelihood from which the baseline hazard function has dropped out (BSA p16).

In Bayesian paradigm, obtaining the posterior distribution of parameters requires the full likelihood function involving all parameters including nuisance ones and the prior for all these parameters. In the case of survival analysis, one of the parameters that requires modeling is the entire baseline hazard function. One way to proceed is to parametrize the baseline hazard function parsimoniously, i.e., parametric survival analysis (BIDA p325, BSA chap 2). The other approach is to more flexibly model the baseline hazard function.

Here we will examine the simplest form of the latter, the piecewise constant hazard model (piecewise exponential model).

## Piecewise constant hazard model
### Likelihood

This model formulation was taken from BSA (p47-).

Firstly, partition the time axis into $J$ intervals using $0 < s_{1} < s_{2} ... < s_{J} < \infty$.

 $$(0,s_{1}], (s_{1},s_{2}], ..., (s_{J-1},s_{J}]$$

$s_{J}$ is a finite value that has to be larger than the largest observed time in the study. Name each interval $I_{j} = (s_{j-1},s_{j}]$. We assume a constant hazard $\lambda$ within each interval. Let $D = (n,\mathbf{y}, X, \nu)$ describe the observed data.

$\mathbf{y} = (y_{1}, ..., y_{n})^{T}$ is the observed times.

$X$ is a $n \times p$ matrix of covariates associated with a length $p$ vector $\boldsymbol{\beta}$.

$\boldsymbol{\nu} = (\nu_{1},...,\nu_{n})^{T}$ is a vector of failure (censoring) indicators.

Let $\boldsymbol{\lambda} = (\lambda_{1},...,\lambda_{J})^{T}$. The full likelihood for $(\boldsymbol{\beta}, \boldsymbol{\lambda})$ is the following.

$$L(\boldsymbol{\beta},\boldsymbol{\lambda} | D) = \prod^{n}_{i=1} \prod^{J}_{j=1} (\lambda_{j} \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}))^{\delta_{ij}\nu_{i}} \exp \left\{ - \delta_{ij} \left[ \lambda_{j}(y_{i} - s_{j-1}) + \sum^{j-1}_{g=1} \lambda_{g}(s_{g} - s_{g-1}) \right] \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}) \right\}$$

where $\delta_{ij}$ is an interval specific indicator of end of follow up indicator $I[y_{i} \in I_{j}]$.

This is really hard, so let us dissect this into more manageable pieces. Firstly, we will focus on one individual rather than the entire dataset.

$$L(\boldsymbol{\beta},\boldsymbol{\lambda} | D_{i}) = \prod^{J}_{j=1} (\lambda_{j} \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}))^{\delta_{ij}\nu_{i}} \exp \left\{ - \delta_{ij} \left[ \lambda_{j}(y_{i} - s_{j-1}) + \sum^{j-1}_{g=1} \lambda_{g}(s_{g} - s_{g-1}) \right] \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}) \right\}$$

Thus, this is a product of interval-specific contributions. However, for interval $I_{j}$ in which the individual just survived without death or censoring $\delta_{ij} = 0$, there is no contribution (no parameters left).

$$(\lambda_{j} \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}))^{(0) \nu_{i}} \exp \left\{ - (0) \left[ \lambda_{j}(y_{i} - s_{j-1}) + \sum^{j-1}_{g=1} \lambda_{g}(s_{g} - s_{g-1}) \right] \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}) \right\} = 1$$

Therefore, the only contribution happens at the interval when the individual either dies or becomes censored. Note in general, an event individual contributes a density, which is a product of hazard and survival. On the other hand, a censored individual can only contribute survival information.


### Event individual contribution

Firstly, we will consider death in interval $I_{j}$.

$$(\lambda_{j} \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}))^{(1) (1)} \exp \left\{ - (1) \left[ \lambda_{j}(y_{i} - s_{j-1}) + \sum^{j-1}_{g=1} \lambda_{g}(s_{g} - s_{g-1}) \right] \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}) \right\}$$

$(\lambda_{j} \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}))^{(1) (1)}$ is the hazard contribution.

$\exp \left\{ - (1) \left[ \lambda_{j}(y_{i} - s_{j-1}) + \sum^{j-1}_{g=1} \lambda_{g}(s_{g} - s_{g-1}) \right] \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}) \right\}$ is the survival contribution. Taken together this individual contributes density information.

Note $\left[ \lambda_{j}(y_{i} - s_{j-1}) + \sum^{j-1}_{g=1} \lambda_{g}(s_{g} - s_{g-1}) \right]$ is the cumulative baseline hazard that this individual faced. By multiplying this with time-constant (by the PH assumption) multiplication factor, $\exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta})$, we obtain the individual-specific cumulative hazard. An exponential of the negative cumulative hazard is the survival.

### Censored individual contribution

Now we will consider censoring in interval $I_{j}$.

$$(\lambda_{j} \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}))^{(1) (0)} \exp \left\{ - (1) \left[ \lambda_{j}(y_{i} - s_{j-1}) + \sum^{j-1}_{g=1} \lambda_{g}(s_{g} - s_{g-1}) \right] \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}) \right\}$$

$(\lambda_{j} \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}))^{(1) (0)} = 1$. There is no hazard contribution.

$\exp \left\{ - (1) \left[ \lambda_{j}(y_{i} - s_{j-1}) + \sum^{j-1}_{g=1} \lambda_{g}(s_{g} - s_{g-1}) \right] \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}) \right\}$ is the survival contribution.

### Poisson trick

This part follows BIDA (p347-) and [Piece-Wise Exponential Model](http://data.princeton.edu/wws509/notes/c7s4.html). Let us examine the likelihood for individual $i$ further. As stated above only contribution happens at the interval in which follow up ends by death or censoring. Without loss of generality, consider this interval as $I_{j}$.

$$
\begin{align*}
L(\boldsymbol{\beta},\boldsymbol{\lambda} | D_{i})
&= (\lambda_{j} \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}))^{\delta_{ij}\nu_{i}} \exp \left\{ - \delta_{ij} \left[ \lambda_{j}(y_{i} - s_{j-1}) + \sum^{j-1}_{g=1} \lambda_{g}(s_{g} - s_{g-1}) \right] \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}) \right\}\\
 &~~~\text{By } \delta_{ij} = 1\\
 &= (\lambda_{j} \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}))^{\nu_{i}} \exp \left\{ - \left[ \lambda_{j}(y_{i} - s_{j-1}) + \sum^{j-1}_{g=1} \lambda_{g}(s_{g} - s_{g-1}) \right] \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}) \right\}\\
 &~~~\text{Let $H_{i,g}$ represent at-risk time during $I_{g}$ for $i$}.\\
 &= (\lambda_{j} \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}))^{\nu_{i}} \exp \left\{ - \left[ \sum^{j}_{g=1} \lambda_{g} H_{i,g} \right] \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}) \right\}\\
 &= (\lambda_{j} \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}))^{\nu_{i}} \exp \left\{ - \sum^{j}_{g=1} \lambda_{g} H_{i,g} \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}) \right\}\\
 &= (\lambda_{j} \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}))^{\nu_{i}} \prod^{j}_{g=1} \exp \left\{ - \lambda_{g} H_{i,g} \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}) \right\}\\
 &~~~\text{Note the new middle piece is 1.}\\
 &= (\lambda_{j} \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}))^{\nu_{i}} \left[ \prod^{j-1}_{g=1} (\lambda_{g} H_{i,g} \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}))^{0} \right] \prod^{j}_{g=1} \exp \left\{ - \lambda_{g} H_{i,g} \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}) \right\}\\
 &\propto (\lambda_{j} H_{i,j} \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}))^{\nu_{i}} \left[ \prod^{j-1}_{g=1} (\lambda_{g} H_{i,g} \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}))^{0} \right] \prod^{j}_{g=1} \exp \left\{ - \lambda_{g} H_{i,g} \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}) \right\}\\
 &~~~\text{By reintroducing } \delta_{i,g} \text{, which is 0 for }g < j\\
 &= \left[ \prod^{j}_{g=1} (\lambda_{g} H_{i,g} \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}))^{\delta_{i,g} \nu_{i}} \right] \prod^{j}_{g=1} \exp \left\{ - \lambda_{g} H_{i,g} \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}) \right\}\\
 &= \prod^{j}_{g=1} \exp \left\{ - \lambda_{g} H_{i,g} \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}) \right\} (\lambda_{g} H_{i,g} \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}))^{\delta_{i,g} \nu_{i}}\\
 &~~~\text{By } \delta_{i,g} \nu_{i} \in \{0,1\}\\
 &= \prod^{j}_{g=1} \exp \left\{ - \lambda_{g} H_{i,g} \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}) \right\} (\lambda_{g} H_{i,g} \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}))^{\delta_{i,g} \nu_{i}} \big/ (\delta_{i,g} \nu_{i})!\\
\end{align*}
$$

We can multiply the likelihood with a term that does not contain parameters and retain the same inference. Note the last expression is a product of individual- and interval-specific Poisson likelihood. This transformation implies that we can split each individual's observation into interval-specific observations up until the interval in which follow up ended. In addition to copying covariates, each interval-specific observation has to have the duration of the at-risk time and an indicator that is 1 in the last interval if the individual died otherwise 0. The latter indicator serves as the outcome of the Poisson model.

The corresponding Poisson model for individual $i$ interval $g$ is the following.

$$
\begin{align*}
\mu_{i,g} &= \lambda_{g} H_{i,g} \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta})\\
\log(\mu_{i,g}) &= \log(\lambda_{g} H_{i,g} \exp(\mathbf{x}_{i}^{T}\boldsymbol{\beta}))\\
 &= \log(H_{i,g}) + \log(\lambda_{g}) + \mathbf{x}_{i}^{T}\boldsymbol{\beta}\\
 &= \log(H_{i,g}) + \beta_{0,g} + \mathbf{x}_{i}^{T}\boldsymbol{\beta}\\
\end{align*}
$$

Therefore, $\log(H_{i,g})$ becomes the offset. The intercept $\beta_{0,g} = \log(\lambda_{g})$ is interval-specific. The outcome, the indicator variable $(\delta_{i,g} \nu_{i})$ is not independent within an individual because it is all zero except the last one, which can be 1 if the individual died. However, the likelihood has the form of a product of interval-specific Poisson contributions. Thus, the Poisson modeling can proceed as if these interval-specific observations from one individual were independent. In R, [survival::survSplit](https://stat.ethz.ch/R-manual/R-devel/library/survival/html/survSplit.html) function can create this long-format dataset from a single-row-per-person dataset.


### Prior specification

We have clarified the likelihood part, so now we need to specify the priors for all parameters. We have the covariate coefficient vector $\boldsymbol{\beta}$. After the Poisson transformation, we also have the interval-specific intercepts. Each one of the parameters can take on any value on the real line.

The Stan developer website has a web page dedicated to prior choice philosophy [(Prior Choice Recommendations)](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations). Here we adopt the principle of weakly informative priors that are informative enough to regularize. The direct quote is the following.


> Weakly informative prior should contain enough information to regularize: the idea is that the prior rules out unreasonable parameter values but is not so strong as to rule out values that might make sense.

#### Priors for covariate coefficients

These coefficients for covariates are on the log hazard ratio scale. Based on the substantive ground, we would like to rule out hazard ratios that are greater than 50 or smaller than 1/50. $\log(50) = 3.912023$. Thus, $N(0,2^2)$ may be a good choice. This prior puts approximately 5% of the prior probability located outside the above stated reasonable range.

#### Priors for baseline hazards

BSA (p48) suggests independent gamma priors for the piecewise baseline hazard parameters and multivariate normal prior for the vector of log baseline hazard parameters. BSA does not mention numerical values. It is hard to think of the reasonable range for the piecewise baseline hazards.

BIDA (p351-) acknowledges this difficulty and suggest centering the priors on an exponential regression model.

$$\lambda_{k} \sim Gamma(\lambda_{*}w_{k}, w_{k})$$

where $w_{k}$ is the interval length times some hyperparameter $w$.


## Data analysis example
### Set up multicore environment

```{r}
## Configure parallelization
## Parallel backend for foreach (also loads foreach and parallel; includes doMC)
library(doParallel)
## Reproducible parallelization
library(doRNG)
## Detect core count (Do not use on clusters)
n_cores <- parallel::detectCores()
## Used by parallel::mclapply() as default
options(mc.cores = n_cores)
## Used by doParallel as default
options(cores = n_cores)
## Register doParallel as the parallel backend with foreach
## http://stackoverflow.com/questions/28989855/the-difference-between-domc-and-doparallel-in-r
doParallel::registerDoParallel(cores = n_cores)
## Report multicore use
## cat("### Using", foreach::getDoParWorkers(), "cores\n")
## cat("### Using", foreach::getDoParName(), "as backend\n")
```

### Load packages

```{r}
library(tidyverse)
library(survival)
library(rstanarm)
```

### Load and prepare dataset

```
aml                  package:survival                  R Documentation
Acute Myelogenous Leukemia survival data
Description:
     Survival in patients with Acute Myelogenous Leukemia.  The
     question at the time was whether the standard course of
     chemotherapy should be extended ('maintainance') for additional
     cycles.
Usage:
     aml
     leukemia
Format:
       time:    survival or censoring time
       status:  censoring status
       x:       maintenance chemotherapy given? (factor)
Source:
     Rupert G. Miller (1997), _Survival Analysis_.  John Wiley & Sons.
     ISBN: 0-471-25218-2.
```

```{r}
data(leukemia, package = "survival")
leukemia <- as_data_frame(leukemia) %>%
    mutate(id = seq_len(n())) %>%
    select(id, everything())
leukemia
```

Check distribution of observed times and decide cut points. This

```{r}
cut_one
cut_two <- c(median(leukemia$time), cut_one)
cut_two
cut_three <- c(quantile(leukemia$time, probs = c(1/3, 2/3)), cut_one)
cut_three
```

Now transform dataset into long-format ones. We need an interval indicator and an interval length variable.

```{r}
## No cut for all same constant hazard
leukemia_one <- survival::survSplit(formula = Surv(time, status) ~ ., data = leukemia, cut = cut_one) %>%
    mutate(interval = as.numeric(factor(tstart)),
           interval_length = time - tstart) %>%
    as_data_frame
leukemia_one
## Split into two observations
leukemia_two <- survival::survSplit(formula = Surv(time, status) ~ ., data = leukemia, cut = cut_two) %>%
    mutate(interval = as.numeric(factor(tstart)),
           interval_length = time - tstart) %>%
    as_data_frame
leukemia_two
## Split into three observations
leukemia_three <- survival::survSplit(formula = Surv(time, status) ~ ., data = leukemia, cut = cut_three) %>%
    mutate(interval = as.numeric(factor(tstart)),
           interval_length = time - tstart) %>%
    as_data_frame
leukemia_three
```

### Sanity check with frequentist methods

```{r}
coxph(formula = Surv(time, status) ~ x,
         data    = leukemia,
         ties    = c("efron","breslow","exact")[1])

glm(formula = status ~ x + offset(log(interval_length)),
    family  = poisson(link = "log"),
    data    = leukemia_one)

glm(formula = status ~ factor(interval) + x + offset(log(interval_length)),
    data = leukemia_two,
    family = poisson(link = "log"))

glm(formula = status ~ factor(interval) + x + offset(log(interval_length)),
    data = leukemia_three,
    family = poisson(link = "log"))
```

### Model with rstanarm

```{r}
fit_leukemia_one <- rstanarm::stan_glm(formula = status ~ x + offset(log(interval_length)),
                                       data = leukemia_one,
                                       family = poisson(link = "log"))
fit_leukemia_one

fit_leukemia_two <- rstanarm::stan_glm(formula = status ~ factor(interval) + x + offset(log(interval_length)),
                                       data = leukemia_two,
                                       family = poisson(link = "log"))
fit_leukemia_two

fit_leukemia_three <- rstanarm::stan_glm(formula = status ~ factor(interval) + x + offset(log(interval_length)),
                                         data = leukemia_three,
                                         family = poisson(link = "log"))
fit_leukemia_three
```
--------------------
- Top Page: http://rpubs.com/kaz_yos/
- Github: https://github.com/kaz-yos
