---
title: "Count Outcome Models with Stan"
author: "Kazuki Yoshida"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output: html_document
---

```{r, message = FALSE, tidy = FALSE, echo = F}
## knitr configuration: http://yihui.name/knitr/options#chunk_options
library(knitr)
showMessage <- FALSE
showWarning <- TRUE
set_alias(w = "fig.width", h = "fig.height", res = "results")
opts_chunk$set(comment = "##", error= TRUE, warning = showWarning, message = showMessage,
               tidy = FALSE, cache = FALSE, echo = TRUE,
               fig.width = 7, fig.height = 7, dev.args = list(family = "sans"))
## for rgl
## knit_hooks$set(rgl = hook_rgl, webgl = hook_webgl)
## for animation
opts_knit$set(animation.fun = hook_ffmpeg_html)
## R configuration
options(width = 116, scipen = 5)
## Record start time
start_time <- Sys.time()
## Configure parallelization
## Parallel backend for foreach (also loads foreach and parallel; includes doMC)
library(doParallel)
## Reproducible parallelization
library(doRNG)
## Detect core count (Do not use on clusters)
n_cores <- parallel::detectCores()
## Used by parallel::mclapply() as default
options(mc.cores = n_cores)
## Used by doParallel as default
options(cores = n_cores)
## Register doParallel as the parallel backend for foreach
## http://stackoverflow.com/questions/28989855/the-difference-between-domc-and-doparallel-in-r
doParallel::registerDoParallel(cores = n_cores)
```

## References
- Web
  - [Stan Negative Binomial Distribution (alternative parameterization)](https://mc-stan.org/docs/2_18/functions-reference/nbalt.html)
  - [(DAE) Negative Binomial Regression | R Data Analysis Examples](https://stats.idre.ucla.edu/r/dae/negative-binomial-regression/)
  - [Negative binomial and mixed Poisson regression](http://www.math.mcgill.ca/~dstephens/523/Papers/Lawless-1987-CJS.pdf)
  - [Writing Stan programs for use with the loo package](http://mc-stan.org/loo/articles/loo2-with-rstan.html)
  - [4.A Models for Over-Dispersed Count Data](https://data.princeton.edu/wws509/stata/overdispersion)
  - [Models for Count Data With Overdispersion](https://data.princeton.edu/wws509/notes/c4a.pdf)
  - [An Introduction to ggdag](https://cran.r-project.org/web/packages/ggdag/vignettes/intro-to-ggdag.html)
- Books
  - [Bayesian Models: a Statistical Primer for Ecologists](https://press.princeton.edu/titles/10523.html)
- Papers
  - Lawless (1987). [Negative binomial and mixed Poisson regression](https://pdfs.semanticscholar.org/83d9/f11d19ded0f1d3e3e4e5e66cb3ae5666189c.pdf)

## Load packages
```{r}
library(tidyverse)
## devtools::install_github('jburos/biostan', build_vignettes = TRUE, dependencies = TRUE)
library(rstan)
## To avoid recompilation of unchanged Stan programs, we recommend calling
rstan_options(auto_write = TRUE)
##
library(bayesplot)
##
library(loo)
## Seed from random.org
set.seed(673788956)
```

## Load data
This dataset contains information on the number of days absent from school for each student along with the student's math test score and the type of program that the student belonged to.
```{r, cache = TRUE}
data1 <- haven::read_dta("https://stats.idre.ucla.edu/stat/stata/dae/nb_data.dta")
```
```{r}
data1 <- data1 %>%
    mutate(prog = factor(prog, levels = 1:3, labels = c("General", "Academic", "Vocational")),
           id = factor(id))
data1
## For counting
data1_count <- data1 %>%
    rename(y = daysabs) %>%
    count(y)
data1_count
```

## Examine distribution
The distribution is quite wide spread.
```{r}
data1 %>%
    ggplot(mapping = aes(x = daysabs, fill = prog)) +
    geom_bar(stat = "count") +
    facet_grid(prog ~ ., margin = TRUE, scales = "free_y") +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5),
          legend.key = element_blank(),
          plot.title = element_text(hjust = 0.5),
          strip.background = element_blank())
```

A rough check of mean-variance comparison indicates presence of overdispersion although this does not account for the continuous math test score variable.
```{r}
data1 %>%
    group_by(prog) %>%
    summarize(mean = mean(daysabs),
              var = var(daysabs))
```

## Design matrix
We will assume the DAE design matrix is sufficient for modeling covariates. A numerical matrix is required for rstan.
```{r}
X <- model.matrix(object = daysabs ~ math + prog, data = data1)
head(X, n = 10)
```
```{r}
y <- data1$daysabs
```

## Helper functions
We will use these function to extract and visualize posterior predictive distributions.
```{r}
##
extract_post_pred <- function(stan_fit) {
    tidybayes::tidy_draws(stan_fit) %>%
        select(.chain, .iteration, .draw, starts_with("y_new")) %>%
        gather(key = key, value = value, starts_with("y_new")) %>%
        mutate(key = gsub("y_new|\\[|\\]", "", key) %>% as.integer())
}
##
plot_draws <- function(stan_fit, n_sample, data_count = data1_count) {
    draw_data <- extract_post_pred(stan_fit)
    sample_indices <- sample(seq_len(max(draw_data$.iteration)), size = n_sample)
    draw_data %>%
        group_by(.chain, .iteration, .draw) %>%
        count(value) %>%
        filter(.iteration %in% sample_indices) %>%
        ggplot(mapping = aes(x = value, y = n, group = interaction(.chain, .iteration, .draw))) +
        ## Plot random draws from posterior
        geom_line(alpha = 0.5, size = 0.1) +
        ## Include actual data distribution
        geom_line(data = data_count, color = "gray", alpha = 0.7, size = 2,
                  mapping = aes(x = y, group = NULL)) +
        facet_wrap( ~ .chain) +
        theme_bw() +
        theme(axis.text.x = element_text(angle = 90, vjust = 0.5),
              legend.key = element_blank(),
              plot.title = element_text(hjust = 0.5),
              strip.background = element_blank())
}
```

## Poisson model without covariates
This is a Poisson model without any covariates. The MCMC diagnostics are good. The model is a poor fit as seen in the discrepancy between posterior predictive distributions and the data distribution.
$$\begin{align*}
  &\text{Prior}\\
  \lambda | \alpha, \beta &\sim Gamma(\alpha, \beta)\\
  \\
  &\text{Likelihood}\\
  y_{i} | \lambda &\overset{\text{iid}}{\sim} Poisson(\lambda)\\
  \end{align*}
$$
```{r}
stan_code_poisson <- biostan::read_stan_file("./bmspe_poisson.stan")
biostan::print_stan_code(stan_code_poisson, section = NULL)
```
```{r, results = "hide"}
stan_model_poisson <- stan(model_code = stan_code_poisson,
                           data = list(a = 10^(-3), b = 10^(-3),
                                       N = nrow(X), M = ncol(X),
                                       X = X,
                                       y = y),
                           chains = n_cores)
```
```{r}
check_hmc_diagnostics(stan_model_poisson)
pars <- c("lambda","lp__")
print(stan_model_poisson, pars = pars)
pairs(stan_model_poisson, pars = pars)
traceplot(stan_model_poisson, inc_warmup = TRUE, pars = pars)
plot_draws(stan_model_poisson, n_sample = 20)
loo(stan_model_poisson)
```

## Poisson model with covariates
Including covariates slightly improved the fit and the posterior predictive distributions are closer to the data distribution. However, the zero part is not accounted for well.
$$\begin{align*}
  &\text{Prior}\\
  \boldsymbol{\beta} &\sim MVN(\boldsymbol{0}, s\mathbf{I})\\
  \\
  &\text{Likelihood}\\
  \eta_{i} &= \mathbf{X}_{i}^{T}\boldsymbol{\beta}\\
  \mu_{i} &= e^{\eta_{i}}\\
  y_{i} | \mu_{i} &\overset{\text{ind}}{\sim} Poisson(\mu_{i})\\
  \end{align*}
$$
```{r}
stan_code_poisson_covs <- biostan::read_stan_file("./bmspe_poisson_covs.stan")
biostan::print_stan_code(stan_code_poisson_covs, section = NULL)
```
```{r, results = "hide"}
stan_model_poisson_covs <- stan(model_code = stan_code_poisson_covs,
                                data = list(s = 10,
                                            N = nrow(X), M = ncol(X),
                                            X = X,
                                            y = y),
                                chains = n_cores)
```
```{r}
check_hmc_diagnostics(stan_model_poisson_covs)
pars <- c("beta","lp__")
print(stan_model_poisson_covs, pars = pars)
pairs(stan_model_poisson_covs, pars = pars)
traceplot(stan_model_poisson_covs, inc_warmup = TRUE, pars = pars)
plot_draws(stan_model_poisson_covs, n_sample = 20)
loo(stan_model_poisson_covs)
```
We are using vague priors on the regression coefficients, so the results are similar to the corresponding Frequentist fit.
```{r}
## Frequentist Poisson fit for comparison
summary(glm(formula = daysabs ~ math + prog, data = data1, family = poisson(link = "log")))
```


## Zero-inflated Poisson model with covariates
The zero-inflated Poisson model a two-part model that models two different types of zeros. Structural zeros coming from a subpopulation who can only have zeros and Poisson zeros coming from subpopulation who can have non-zero values but by chance had zeros. In this example of days absent from school, this may not be a feasible model.
$$\begin{align*}
  &\text{Prior}\\
  \boldsymbol{\beta} &\sim MVN(\boldsymbol{0}, \text{diag}(s))\\
  \boldsymbol{\beta}_{\theta} &\sim MVN(\boldsymbol{0}, \text{diag}(s_{\theta}))\\
  \\
  &\text{Likelihood}\\
  \eta_{\theta,i} &= \mathbf{X}_{i}^{T}\boldsymbol{\beta}_{\theta}\\
  \mu_{\theta,i} &= \text{expit}(\eta_{i})\\
  z_{i} | \mu_{\theta,i} &\overset{\text{ind}}{\sim} Bernoulli(\mu_{\theta,i})\\
  \\
  \eta_{i} &= \mathbf{X}_{i}^{T}\boldsymbol{\beta}\\
  \mu_{i} &= e^{\eta_{i}}\\
  y_{i} | \mu_{i},z_{i} &\overset{\text{ind}}{\sim} (1 - z_{i}) Poisson(\mu_{i})\\
  \end{align*}
$$
```{r}
stan_code_poisson_covs_zip <- biostan::read_stan_file("./bmspe_poisson_covs_zip.stan")
biostan::print_stan_code(stan_code_poisson_covs_zip, section = NULL)
```
```{r, results = "hide"}
stan_model_poisson_covs_zip <- stan(model_code = stan_code_poisson_covs_zip,
                                    ## Regularize binomial part to avoid divergent iterations.
                                    data = list(s = rep(10,ncol(X)), s_theta = rep(10,ncol(X)),
                                                N = nrow(X), M = ncol(X),
                                                X = X,
                                                y = y),
                                    chains = n_cores)
```
```{r}
check_hmc_diagnostics(stan_model_poisson_covs_zip)
pars <- c("beta","beta_theta","lp__")
print(stan_model_poisson_covs_zip, pars = pars)
pairs(stan_model_poisson_covs_zip, pars = pars)
traceplot(stan_model_poisson_covs_zip, inc_warmup = TRUE, pars = pars)
plot_draws(stan_model_poisson_covs_zip, n_sample = 20)
loo(stan_model_poisson_covs_zip)
```
The model did not converge with vague priors on both sets of coefficients. The pairs plot shows straight line patterns among coefficients fro the Bernoulli model coefficients. As seen in the barplots of the data, the General program does not have any individuals with zero days absent from school. This may be causing a complete separation issue. Taking this as a data sparsity issue, we will examine a model that regularizes more heavily the non-intercept coefficients for the Bernoulli model.
```{r, results = "hide"}
stan_model_poisson_covs_zip2 <- stan(model_code = stan_code_poisson_covs_zip,
                                     ## Regularize binomial part to avoid divergent iterations.
                                     data = list(s = rep(10,ncol(X)), s_theta = c(10,rep(2, ncol(X)-1)),
                                                 N = nrow(X), M = ncol(X),
                                                 X = X,
                                                 y = y),
                                     chains = n_cores)
```
```{r}
check_hmc_diagnostics(stan_model_poisson_covs_zip2)
pars <- c("beta","beta_theta","lp__")
print(stan_model_poisson_covs_zip2, pars = pars)
pairs(stan_model_poisson_covs_zip2, pars = pars)
traceplot(stan_model_poisson_covs_zip2, inc_warmup = TRUE, pars = pars)
plot_draws(stan_model_poisson_covs_zip2, n_sample = 20)
loo(stan_model_poisson_covs_zip2)
```
The regularized model converged. The narrow linear patterns among the Bernoulli model coefficients are probably due to strong correlation. Being either in an Academic program (coefficient 3) or Vocational program (coefficient 4) is a strong predictor of having zero absent days. So when one is strong, the other is also strong. When the coefficients for these are strong, the intercept (coefficient 1) has to be very small.
```{r}
## Frequentist ZIP fit for comparison
## https://stats.idre.ucla.edu/r/dae/zip/
summary(pscl::zeroinfl(formula = daysabs ~ math + prog | math + prog, data = data1))
```
As expected, a non-regularized Frequentist model breaks down with very high standard errors for Bernoulli model coefficients.


## Poisson model with covariates and gamma random effects
$$\begin{align*}
  &\text{Prior}\\
  \boldsymbol{\beta} &\sim MVN(\boldsymbol{0}, s\mathbf{I})\\
  a_{\gamma} &\sim Gamma(a, b)\\
  \\
  &\text{Likelihood}\\
  \eta_{i} &= \mathbf{X}_{i}^{T}\boldsymbol{\beta}\\
  \mu_{i} &= e^{\eta_{i}}\\
  \gamma_{i} | a_{\gamma} &\overset{\text{iid}}{\sim} Gamma(1/a_{\gamma},1/a_{\gamma}) ~~ E[\gamma_{i}] = 1, Var(\gamma_{i}) = a_{\gamma}\\
  y_{i} | \gamma_{i},\mu_{i} &\overset{\text{ind}}{\sim} Poisson(\gamma_{i} \mu_{i})\\
  \end{align*}
$$
```{r}
stan_code_poisson_covs_gamma <- biostan::read_stan_file("./bmspe_poisson_covs_gamma.stan")
biostan::print_stan_code(stan_code_poisson_covs_gamma, section = NULL)
```
```{r, results = "hide"}
stan_model_poisson_covs_gamma <- stan(model_code = stan_code_poisson_covs_gamma,
                                      data = list(a = 10^(-3), b = 10^(-3),
                                                  s = 10,
                                                  N = nrow(X), M = ncol(X),
                                                  X = X,
                                                  y = y),
                                      chains = n_cores)
```
```{r}
check_hmc_diagnostics(stan_model_poisson_covs_gamma)
pars <- c("beta","a_gamma","lp__")
print(stan_model_poisson_covs_gamma, pars = pars)
pairs(stan_model_poisson_covs_gamma, pars = pars)
traceplot(stan_model_poisson_covs_gamma, inc_warmup = TRUE, pars = pars)
plot_draws(stan_model_poisson_covs_gamma, n_sample = 20)
loo(stan_model_poisson_covs_gamma)
```

## Negative binomial model with covariates
If we marginalize over the latent individual-specific random effect $\gamma_{i}$, we obtain the negative binomial. In the following formulation, $a_{\gamma}$ should have the same interpretation as in the previous model.
$$\begin{align*}
  &\text{Prior}\\
  \boldsymbol{\beta} &\sim MVN(\boldsymbol{0}, s\mathbf{I})\\
  a_{\gamma} &\sim Gamma(a, b)\\
  \phi &= \frac{1}{a_{\gamma}}\\
  \\
  &\text{Likelihood}\\
  \eta_{i} &= \mathbf{X}_{i}^{T}\boldsymbol{\beta}\\
  \mu_{i} &= e^{\eta_{i}}\\
  y_{i} | \mu_{i}, \phi &\overset{\text{ind}}{\sim} NegBin(\mu_{i}, \phi)\\
  \\
  &\text{Stan definition}\\
  \text{NegBinomial2}(y_{i} | \mu_{i}, \phi) &= \frac{\Gamma(y_{i} + \phi)}{y_{i}!\Gamma(\phi)} \left( \frac{\mu_{i}}{\mu_{i}+\phi} \right)^{y_{i}} \left( \frac{\phi}{\mu_{i}+\phi} \right)^{\phi}\\
  \end{align*}
$$
```{r}
## http://rstudio-pubs-static.s3.amazonaws.com/34099_2e35c3966ef548c2918d5b6c2146bfd1.html
stan_code_negbin_covs <- biostan::read_stan_file("./bmspe_negbin_covs.stan")
biostan::print_stan_code(stan_code_negbin_covs, section = NULL)
```
```{r, results = "hide"}
stan_model_negbin_covs <- stan(model_code = stan_code_negbin_covs,
                               data = list(a = 10^(-3), b = 10^(-3),
                                           s = 10,
                                           N = nrow(X), M = ncol(X),
                                           X = X,
                                           y = y),
                                chains = n_cores)
```
```{r}
check_hmc_diagnostics(stan_model_negbin_covs)
pars <- c("beta","a_gamma","phi","lp__")
print(stan_model_negbin_covs, pars = pars)
pairs(stan_model_negbin_covs, pars = pars)
traceplot(stan_model_negbin_covs, inc_warmup = TRUE, pars = pars)
plot_draws(stan_model_negbin_covs, n_sample = 20)
loo(stan_model_negbin_covs)
```
We see the similarity in the posterior for $a_{\gamma}$ as well as the posterior predictive plots, empirically confirming the equivalence of these two models.

```{r}
## Frequentist negative binomial fit for comparison
summary(MASS::glm.nb(formula = daysabs ~ math + prog, data = data1))
```
Due to the vague prior, the Frequentist fit is similar.


## Comparison of Poisson with gamma random effects and negative binomial
The corresponding posterior samples are very similar between these two models.
```{r}
cat("Poisson with gamma random effects\n")
print(stan_model_poisson_covs_gamma, pars = c("beta","a_gamma"))
cat("Negative binomial\n")
print(stan_model_negbin_covs, pars = c("beta","a_gamma"))
```
Figures may be more informative.
```{r}
both_draws <- bind_rows(tidybayes::tidy_draws(stan_model_poisson_covs_gamma) %>%
                        select(.chain, .iteration, .draw, starts_with("beta"), a_gamma) %>%
                        gather(key = key, value = value, starts_with("beta"), a_gamma) %>%
                        mutate(model = "Poisson Mixed"),
                        ##
                        tidybayes::tidy_draws(stan_model_negbin_covs) %>%
                        select(.chain, .iteration, .draw, starts_with("beta"), a_gamma) %>%
                        gather(key = key, value = value, starts_with("beta"), a_gamma) %>%
                        mutate(model = "Negative Binomial"))

both_draws %>%
    ggplot(mapping = aes(x = value, color = model)) +
    geom_density() +
    facet_wrap( ~ key, scales = "free") +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5),
          legend.key = element_blank(),
          plot.title = element_text(hjust = 0.5),
          strip.background = element_blank())

both_draws %>%
    ggplot(mapping = aes(y = value, x = model, color = model)) +
    geom_boxplot() +
    facet_wrap( ~ key, scales = "free") +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5),
          legend.key = element_blank(),
          plot.title = element_text(hjust = 0.5),
          strip.background = element_blank())

both_draws %>%
    ggplot(mapping = aes(y = value, x = model, color = model)) +
    geom_violin() +
    facet_wrap( ~ key, scales = "free") +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5),
          legend.key = element_blank(),
          plot.title = element_text(hjust = 0.5),
          strip.background = element_blank())

both_draws %>%
    ggplot(mapping = aes(y = value, x = model, color = model)) +
    tidybayes::geom_eye() +
    facet_wrap( ~ key, scales = "free") +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5),
          legend.key = element_blank(),
          plot.title = element_text(hjust = 0.5),
          strip.background = element_blank())
```
Posterior predictive distributions can also be compared. They look somewhat more different.
```{r}
both_ppd <- bind_rows(extract_post_pred(stan_model_poisson_covs_gamma) %>%
                      mutate(model = "Poisson Mixed"),
                      extract_post_pred(stan_model_negbin_covs) %>%
                      mutate(model = "Negative Binomial"))

both_ppd %>%
    group_by(model, .chain, .iteration, .draw) %>%
    count(value) %>%
    group_by(model, value) %>%
    tidybayes::median_qi(n, .width = c(0.95, 0.80, 0.50)) %>%
    ggplot(mapping = aes(x = value, y = n)) +
    tidybayes::geom_interval() +
    geom_point(data = data1_count,
               mapping = aes(x = y, y = n)) +
    geom_line(data = data1_count,
               mapping = aes(x = y, y = n)) +
    scale_color_brewer() +
    facet_wrap( ~ model) +
    coord_cartesian(xlim = c(0, 50)) +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5),
          legend.key = element_blank(),
          plot.title = element_text(hjust = 0.5),
          strip.background = element_blank())

both_ppd_qi <- both_ppd %>%
    group_by(model, .chain, .iteration, .draw) %>%
    count(value) %>%
    group_by(model, value) %>%
    tidybayes::median_qi(n, .width = c(0.95, 0.80, 0.50))

both_ppd_qi %>%
    ggplot(mapping = aes(x = value, y = n)) +
    geom_line() +
    geom_ribbon(data = filter(both_ppd_qi, .width == 0.95),
                mapping = aes(ymin = .lower, ymax = .upper),
                fill = "gray95") +
    geom_ribbon(data = filter(both_ppd_qi, .width == 0.80),
                mapping = aes(ymin = .lower, ymax = .upper),
                fill = "gray80") +
    geom_ribbon(data = filter(both_ppd_qi, .width == 0.50),
                mapping = aes(ymin = .lower, ymax = .upper),
                fill = "gray50") +
    geom_point(data = data1_count,
               mapping = aes(x = y, y = n)) +
    geom_line(data = data1_count,
               mapping = aes(x = y, y = n)) +
    facet_wrap( ~ model) +
    coord_cartesian(xlim = c(0, 50)) +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5),
          legend.key = element_blank(),
          plot.title = element_text(hjust = 0.5),
          strip.background = element_blank())
```


## Comparison of Poisson with negative binomial
Now we compare the goodness of fit for the Poisson model and negative binomial model.
```{r}
cat("Poisson\n")
print(stan_model_poisson_covs, pars = c("beta"))
cat("Negative binomial\n")
print(stan_model_negbin_covs, pars = c("beta","a_gamma"))
```
As seen here posterior means are very similar for the corresponding parameters. Again figures may be more informative.
```{r}
both_draws <- bind_rows(tidybayes::tidy_draws(stan_model_poisson_covs) %>%
                        select(.chain, .iteration, .draw, starts_with("beta")) %>%
                        gather(key = key, value = value, starts_with("beta")) %>%
                        mutate(model = "Poisson"),
                        ##
                        tidybayes::tidy_draws(stan_model_negbin_covs) %>%
                        select(.chain, .iteration, .draw, starts_with("beta"), a_gamma) %>%
                        gather(key = key, value = value, starts_with("beta"), a_gamma) %>%
                        mutate(model = "Negative Binomial"))

both_draws %>%
    ggplot(mapping = aes(x = value, color = model)) +
    geom_density() +
    facet_wrap( ~ key, scales = "free") +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5),
          legend.key = element_blank(),
          plot.title = element_text(hjust = 0.5),
          strip.background = element_blank())

both_draws %>%
    ggplot(mapping = aes(y = value, x = model, color = model)) +
    geom_boxplot() +
    facet_wrap( ~ key, scales = "free") +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5),
          legend.key = element_blank(),
          plot.title = element_text(hjust = 0.5),
          strip.background = element_blank())

both_draws %>%
    ggplot(mapping = aes(y = value, x = model, color = model)) +
    geom_violin() +
    facet_wrap( ~ key, scales = "free") +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5),
          legend.key = element_blank(),
          plot.title = element_text(hjust = 0.5),
          strip.background = element_blank())

both_draws %>%
    ggplot(mapping = aes(y = value, x = model, color = model)) +
    tidybayes::geom_eye() +
    facet_wrap( ~ key, scales = "free") +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5),
          legend.key = element_blank(),
          plot.title = element_text(hjust = 0.5),
          strip.background = element_blank())
```
Now we can better appreciate the more wide spread posteriors for the corresponding parameters.

Posterior predictive distributions can also be compared.
```{r}
both_ppd <- bind_rows(extract_post_pred(stan_model_poisson_covs) %>%
                      mutate(model = "Poisson"),
                      extract_post_pred(stan_model_negbin_covs) %>%
                      mutate(model = "Negative Binomial"))

both_ppd %>%
    group_by(model, .chain, .iteration, .draw) %>%
    count(value) %>%
    group_by(model, value) %>%
    tidybayes::median_qi(n, .width = c(0.95, 0.80, 0.50)) %>%
    ggplot(mapping = aes(x = value, y = n)) +
    tidybayes::geom_interval() +
    geom_point(data = data1_count,
               mapping = aes(x = y, y = n)) +
    geom_line(data = data1_count,
               mapping = aes(x = y, y = n)) +
    scale_color_brewer() +
    facet_wrap( ~ model) +
    coord_cartesian(xlim = c(0, 50)) +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5),
          legend.key = element_blank(),
          plot.title = element_text(hjust = 0.5),
          strip.background = element_blank())

both_ppd_qi <- both_ppd %>%
    group_by(model, .chain, .iteration, .draw) %>%
    count(value) %>%
    group_by(model, value) %>%
    tidybayes::median_qi(n, .width = c(0.95, 0.80, 0.50))

both_ppd_qi %>%
    ggplot(mapping = aes(x = value, y = n)) +
    geom_line() +
    geom_ribbon(data = filter(both_ppd_qi, .width == 0.95),
                mapping = aes(ymin = .lower, ymax = .upper),
                fill = "gray95") +
    geom_ribbon(data = filter(both_ppd_qi, .width == 0.80),
                mapping = aes(ymin = .lower, ymax = .upper),
                fill = "gray80") +
    geom_ribbon(data = filter(both_ppd_qi, .width == 0.50),
                mapping = aes(ymin = .lower, ymax = .upper),
                fill = "gray50") +
    geom_point(data = data1_count,
               mapping = aes(x = y, y = n)) +
    geom_line(data = data1_count,
               mapping = aes(x = y, y = n)) +
    facet_wrap( ~ model) +
    coord_cartesian(xlim = c(0, 50)) +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5),
          legend.key = element_blank(),
          plot.title = element_text(hjust = 0.5),
          strip.background = element_blank())
```
In this case, the Poisson model fits data apparently poorly. We can compare these models more formally using WAIC.
```{r}
cat("WAIC Poisson")
waic_stan_model_poisson_covs <- loo::waic(loo::extract_log_lik(stan_model_poisson_covs))
waic_stan_model_poisson_covs
cat("WAIC negative binomial")
waic_stan_model_negbin_covs <- loo::waic(loo::extract_log_lik(stan_model_negbin_covs))
waic_stan_model_negbin_covs
cat("WAIC comparison")
loo::compare(waic_stan_model_poisson_covs,
             waic_stan_model_negbin_covs)
```
The expected log pointwise predictive density (elpd) is higher for the negative binomial model, suggesting a better fit.

We can also use loo.
```{r}
cat("LOO Poisson")
loo_stan_model_poisson_covs <- loo::loo(loo::extract_log_lik(stan_model_poisson_covs))
loo_stan_model_poisson_covs
cat("LOO negative binomial")
loo_stan_model_negbin_covs <- loo::loo(loo::extract_log_lik(stan_model_negbin_covs))
loo_stan_model_negbin_covs
cat("LOO comparison")
loo::compare(loo_stan_model_poisson_covs,
             loo_stan_model_negbin_covs)
```
The results seem similar in this case.

--------------------
- Top Page: http://rpubs.com/kaz_yos/
- Github: https://github.com/kaz-yos

```{r}
print(sessionInfo())
## Record execution time and multicore use
end_time <- Sys.time()
diff_time <- difftime(end_time, start_time, units = "auto")
cat("Started  ", as.character(start_time), "\n",
    "Finished ", as.character(end_time), "\n",
    "Time difference of ", diff_time, " ", attr(diff_time, "units"), "\n",
    "Used ", foreach::getDoParWorkers(), " cores\n",
    "Used ", foreach::getDoParName(), " as backend\n",
    sep = "")
```
